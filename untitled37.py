# -*- coding: utf-8 -*-
"""Untitled37.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yjmPfj3H0VwANdK2E9Ln6GFNXHKxgPAT
"""

import streamlit as st
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Define the model ID for LLaMA-2
model_id = "NousResearch/Llama-2-7b-chat-hf"

# Load the configuration and model
config = PeftConfig.from_pretrained('MassMin/llama2_ai_medical_chatbot')
model = AutoModelForCausalLM.from_pretrained(model_id)
model = PeftModel.from_pretrained(model, 'MassMin/llama2_ai_medical_chatbot')

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# Initialize the pipeline for text generation
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=256
)

# Streamlit interface
st.title("AI Medical Chatbot")

# User input section
user_input = st.text_input("Enter your medical question:")

# Generate response when the user provides input
if user_input:
    with st.spinner("Generating response..."):
        result = pipe(f"<s>[INST] {user_input} [/INST]")
        st.write(result[0]['generated_text'])

# Add an information section at the bottom
st.info("Disclaimer: This chatbot provides general medical information and is not a substitute for professional medical advice.")

!pip install peft

